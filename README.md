
# Sign Language Detector 

A real-time American Sign Language (ASL) detection web app using MediaPipe Hands and a custom-trained TensorFlow.js model.

Built with:
- React + Vite âš›ï¸âš¡
- TypeScript ğŸ› 
- Tailwind CSS ğŸ¨
- TensorFlow.js ğŸ§ 
- MediaPipe Hands âœ‹
- Vercel Deployment (soon)ğŸš€

## ğŸŒ Live Demo
ğŸ‘‰ (soon)

## âœ¨ Features

- ğŸ“¸ Real-time hand sign detection using webcam
- ğŸ–¼ Upload image-based ASL predictions
- ğŸ¯ Model trained on custom MediaPipe landmark data
- âš¡ Fast inference with TensorFlow.js
- ğŸ¨ Responsive modern UI (Tailwind + Lucide icons)

## ğŸ›  How It Works

1. MediaPipe Hands detects 21 hand landmarks (x, y, z)
2. Extracted landmarks are fed into a trained neural network
3. The model classifies the gesture as an ASL letter or action (`A-Z`, `space`, `del`, `nothing`)

## ğŸš€ Deployment

This project is deployed using **Vercel**.  
Every push to `main` automatically redeploys.

To deploy manually:

```bash
# Build
npm run build

# Preview locally
npm run preview
```

ğŸ“‚ Folder Structure

â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ ImageUpload.tsx  // Main detection logic
â”‚   â”œâ”€â”€ assets/              // Icons, logo, etc.
â”‚   â”œâ”€â”€ App.tsx              // Entry UI
â”œâ”€â”€ public/
â”‚   â””â”€â”€ model.json           // Exported TensorFlow.js model
â”œâ”€â”€ README.md

ğŸ§  Model Training

Model was trained using custom .npy landmark files for each ASL sign.

Trained and exported with:
	â€¢	TensorFlow 2.15.0
	â€¢	Converted to TensorFlow.js via tensorflowjs_converter

ğŸ“¸ Example

<img src="example.png" width="500" />


ğŸ‘¨â€ğŸ’» Author
	â€¢	Faris Alisic
	â€¢	GitHub: @FarisAlisic
