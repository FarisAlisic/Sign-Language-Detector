
# Sign Language Detector 

A real-time American Sign Language (ASL) detection web app using MediaPipe Hands and a custom-trained TensorFlow.js model.

Built with:
- React + Vite âš›ï¸âš¡
- TypeScript ğŸ› 
- Tailwind CSS ğŸ¨
- TensorFlow.js ğŸ§ 
- MediaPipe Hands âœ‹
- Vercel Deployment (soon)ğŸš€

## ğŸŒ Live Demo
ğŸ‘‰ (soon)

## âœ¨ Features

- ğŸ“¸ Real-time hand sign detection using webcam
- ğŸ–¼ Upload image-based ASL predictions
- ğŸ¯ Model trained on custom MediaPipe landmark data
- âš¡ Fast inference with TensorFlow.js
- ğŸ¨ Responsive modern UI (Tailwind + Lucide icons)

## ğŸ›  How It Works

1. MediaPipe Hands detects 21 hand landmarks (x, y, z)
2. Extracted landmarks are fed into a trained neural network
3. The model classifies the gesture as an ASL letter or action (`A-Z`, `space`, `del`, `nothing`)

## ğŸš€ Deployment

Navigate to project folder,to deploy manually:

```bash
# Build
npm run build

# Preview locally
npm run preview
```

ğŸ§  Model Training

Model was trained using custom .npy landmark files for each ASL sign.

Trained and exported with:
	â€¢	TensorFlow 2.15.0
	â€¢	Converted to TensorFlow.js via tensorflowjs_converter

ğŸ“¸ Example

<img src="example.png" width="500" />


ğŸ‘¨â€ğŸ’» Author
	â€¢	Faris Alisic
	â€¢	GitHub: @FarisAlisic
